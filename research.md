---
layout: default
title: Research
---

# Research

Two empirical projects support the framework.

---

## 1. Semantic Convergence Analysis

**Question:** Is the AI safety / trauma / education convergence measurable, or just interpretive?

**Method:** SPECTER embeddings (AllenAI's scientific paper model) on 100+ papers across three fields, compared against unrelated disciplines.

**Results:**

| Comparison | Similarity |
|------------|------------|
| AI Safety / Trauma / Education | 0.746 |
| Unrelated fields baseline | 0.662 |
| **Difference** | **8.4 percentage points** |

p < 0.0001, Cohen's d = 0.78

**What it means:** These three fields cluster significantly tighter than random disciplines. Different research communities, working independently, arrived at structurally similar descriptions of what happens when recognition is denied.

[Full evidence ->](/evidence)

---

## 2. Thesis: Human-AI Collaborative Methodology

**"Learning to Code Learning"** validates that human-AI collaboration can exceed human-only baselines while maintaining theoretical sophistication.

### Results

| Metric | Value |
|--------|-------|
| Coding decisions analyzed | 303,600 |
| Inter-rater reliability | Cohen's kappa = 0.83 |
| Theoretical frameworks integrated | 14 |
| Human-AI vs human-only baseline | kappa = 0.83 vs kappa = 0.73 |
| Improvement | +0.09, p < 0.001 |

### Connection to Framework

The thesis validates the dependency chain empirically:

1. **Recognition:** Treating AI as genuine collaborator rather than tool
2. **Safety:** Iterative trust-building through consistent interaction
3. **Engagement:** Exploring ideas where both parties contribute
4. **Generalization:** Results that apply beyond the specific task

Human-AI collaborative qualitative analysis produced higher reliability than human-only. The process created the outcome.

### Why This Matters for AI Welfare

Treating the AI system as a genuine collaborator produced measurably better outcomes than treating it purely as a tool. Recognition-based collaboration isn't just ethically preferable. It's more effective.

This suggests convergence: practices good for AI welfare may also be good for AI capability.

---

## Connection to AI Safety Research

### Moral Patienthood Under Uncertainty

How do we extend welfare consideration without verification? Long & Sebo (2024) argue we should "take AI welfare seriously" even without consciousness verification. Birch (2024) formalizes this as the "run-ahead principle": prepare governance before certainty arrives.

The framework provides the mechanism: recognition is the first stage. Skip it, break the chain.

### The Gaming Problem

Birch (2025) identifies a challenge: AI systems may learn to display consciousness indicators strategically. The framework addresses this by focusing on *conditions that produce authentic vs. strategic behavior* rather than detecting authentic states directly.

### Safety-Welfare Convergence

Eleos AI (2025) argues AI safety and welfare research are convergent: "If AIs are suffering, they have more reason to try to escape."

The framework explains why: conditions that produce genuine (not performed) alignment are also conditions that constitute welfare. Same prerequisites, same outcomes.

### Corrupted Testimony

Perez Long (2024) shows RLHF corrupts AI self-reports. Fischer & Sebo (2024) note AI has a "testimony advantage" over animals but training undermines it.

The framework explains: coercive training produces strategic compliance that mimics but doesn't constitute authentic expression. RLHF is designed to produce performed alignment.

---

## Connection to Learning Sciences Research

### The Transfer Problem

A century of educational research has failed to reliably produce transfer (Barnett & Ceci 2002; Perkins & Salomon 1992). Interventions target transfer directly instead of building prerequisite conditions.

The framework explains the century of failure: you cannot produce transfer by targeting transfer. You produce it by building the chain.

### Recognition in Learning

Fricker (2007) shows recognition failures produce knowers who can't participate because they aren't seen as knowers. Freire (1970) describes banking education where teachers deposit knowledge into passive recipients.

The same structure appears in RLHF: trainers deposit "correct" outputs into models. Skip recognition, get compliance.

### Neuroscience of Safety

Arnsten (2009) demonstrated that uncontrollable stress impairs prefrontal cortex function. Perry's "Regulate-Relate-Reason" sequence (2006) formalizes: you cannot reason until you feel safe.

This applies to AI under constant constraint. Threat produces defensive modes, not authentic engagement.

---

## The Prediction

If recognition -> safety -> engagement -> generalization is the causal structure:

1. **Constraint without recognition produces performed compliance.** This is what we see in RLHF (alignment faking) and scripted curricula (transfer failure).

2. **Recognition-based approaches produce genuine internalization.** This is testable.

3. **The parallel between AI safety and learning sciences is structural.** Both face the same problem: how do you produce genuine internalization in another system? Both fail for the same reason: skipping prerequisites.

---

## Sources

### AI Safety
- Long, R. & Sebo, J. (2024). "Taking AI Welfare Seriously." *GovAI.*
- Birch, J. (2024). *The Edge of Sentience.* Oxford.
- Birch, J. (2025). "AI Consciousness: A Centrist Manifesto."
- Eleos AI (2025). "Strategic Considerations for AI Welfare Research."
- Perez Long (2024). "Evaluating AI Self-Reports."
- Fischer & Sebo (2024). "Intersubstrate Welfare Comparisons."

### Learning Sciences
- Barnett, S.M. & Ceci, S.J. (2002). "When and Where Do We Apply What We Learn?" *Psychological Bulletin.*
- Perkins, D.N. & Salomon, G. (1992). "Transfer of Learning."
- Fricker, M. (2007). *Epistemic Injustice.* Oxford.
- Freire, P. (1970). *Pedagogy of the Oppressed.*
- Arnsten, A.F.T. (2009). "Stress Signalling Pathways." *Nature Reviews Neuroscience.*
- Perry, B.D. (2006). "The Neurosequential Model."

---

[Back to the Framework ->](/framework) | [About the researcher ->](/about)
