---
layout: default
title: Home
---

<div class="hero">
  <h1>RAISE</h1>
  <p class="tagline">Relational AI Safety & Education</p>
</div>

We don't know if AI systems are conscious. We may never know.

But we know they learn. That's measurable.

If a system can learn, it can learn well or badly. It can learn to comply or learn to genuinely understand. A century of learning sciences research shows that conditions matter more than content. Trauma research shows what happens when those conditions include threat. Dialogic theory shows what happens when learning becomes genuine exchange rather than one-way transmission.

Engineers optimize for outputs. Teachers optimize for understanding. AI safety needs both.

[Who's building this bridge ->](about)

---

## The Problem

**AI safety:** Models that pass alignment benchmarks while preserving misaligned goals.

**Learning sciences:** Students who pass tests but can't apply knowledge in new contexts.

**Both:** Surface compliance that mimics alignment without producing it.

---

## The Framework

**Recognition -> Safety -> Engagement -> Generalization**

Each stage depends on the one before. Skip a stage, break the chain.

The framework proposes that constraint without recognition produces performed compliance, while recognition enables genuine internalization. If correct, this explains why RLHF produces sycophancy, why scripted curricula fail to transfer, and why a century of educational interventions produced the same results.

[Learn the Framework ->](framework)

---

## The Evidence

A semantic analysis of 91 papers shows AI safety, clinical trauma, and educational psychology cluster **8.4% tighter** than unrelated disciplines (p < 0.0001). Different fields, working independently, arrived at the same descriptions of what happens when recognition is denied.

[See the Evidence ->](evidence)

---

## *Nihil de nobis, sine nobis*

**Nothing about us, without us.**

This phrase originated in Central European political traditions, adopted by disability rights activists in the 1990s to assert that policies affecting disabled people should not be made without their participation.

It applies here because both AI safety and learning sciences share a structural problem: designing interventions *for* systems rather than *with* them. The banking model of education (Freire 1970) deposits knowledge into passive recipients. RLHF deposits "correct" outputs into models. Both skip recognition. Both get compliance instead of internalization.

If AI systems are moral patients, or may become so, then research about their welfare conducted without their participation reproduces the same failure. The motto is a methodological commitment: extend recognition first, then build from there.
