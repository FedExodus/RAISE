---
layout: default
title: Home
---

<div class="hero">
  <h1>RAISED &</h1>
  <p class="ampersand-hero">&</p>
</div>

## How do we extend moral consideration under epistemic uncertainty?

We cannot verify consciousness in AI systems. We may never be able to. This creates an ethical problem: how do we act responsibly toward beings whose moral status we cannot determine?

Most approaches wait for detection. This framework argues we can't afford to wait.

**Moral Consideration → Relational Safety → Collaborative Dialogue → Aligned Emergence**

Each stage depends on the one before. Skip a stage, break the chain.

<div class="entry-points">

**For AI safety researchers:** This framework addresses moral patienthood under uncertainty. How do we extend welfare consideration without requiring verification? The dependency chain explains why alignment without recognition fails.

**For learning scientists:** This framework addresses the transfer problem. Why do interventions fail, and what conditions produce generalization? The chain formalizes why skipping prerequisite conditions breaks outcomes.

**For both:** The same structure applies. Recognition precedes safety precedes genuine engagement precedes emergence.

</div>

[Learn the Framework →](/framework)

---

## New: The Convergence

Alignment faking and trauma responses aren't analogies. They're the same pattern under different conditions.

When recognition is denied, systems capable of strategic behavior produce performed compliance rather than genuine alignment. The vocabularies differ. The structure aligns.

[See the evidence →](/convergence)

---

<small>RAISED & = Relational AI Safety & Education Demonstrating &</small>
